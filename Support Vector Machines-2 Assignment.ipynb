{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4124d25-748a-4682-b026-334359f13ddf",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "    In machine learning algorithms, polynomial functions and kernel functions are related through the concept of feature space transformation. Both polynomial functions and kernel functions are used to implicitly map the data points into higher-dimensional feature spaces, enabling the algorithms to learn complex patterns and decision boundaries that may not be possible to represent in the original feature space.\n",
    "\n",
    "    Polynomial Functions:\n",
    "    Polynomial functions are straightforward mathematical functions that involve raising a given input to a certain power. For example, a simple 1D polynomial function could be f(x) = x^2, which takes an input x and maps it to x squared in the feature space. In a 2D space, a 2D polynomial function could be f(x, y) = (x^2, y^2), which maps each (x, y) point to a new point (x^2, y^2) in a higher-dimensional space.\n",
    "    In machine learning, polynomial features are used to transform the original feature space to a higher-dimensional space to handle non-linear relationships between features. For example, in polynomial regression, a linear regression model can be enhanced by including polynomial features of the input variables to represent non-linear patterns.\n",
    "\n",
    "    Kernel Functions:\n",
    "    Kernel functions, on the other hand, are used in kernel-based algorithms, such as Support Vector Machines (SVM), to implicitly compute the dot product between the data points in the higher-dimensional feature space without explicitly transforming the data points. The kernel function K(xi, xj) takes two data points xi and xj from the original feature space and implicitly maps them into the higher-dimensional space.\n",
    "    The key idea behind kernel functions is to avoid the computational overhead of explicitly transforming the data into higher-dimensional spaces while still capturing the higher-dimensional interactions between the data points. This is known as the \"kernel trick.\" By using kernel functions, the algorithms can operate directly in the original feature space while effectively learning complex decision boundaries in the transformed space.\n",
    "\n",
    "    Relationship:\n",
    "    The relationship between polynomial functions and kernel functions lies in the fact that certain kernel functions can represent the same feature space transformations as specific polynomial functions. In other words, some kernel functions can mimic the effects of polynomial feature mappings without explicitly computing them.\n",
    "\n",
    "    For instance, the Polynomial Kernel is a common kernel used in SVM, and it represents the feature space transformation induced by a polynomial function. The Polynomial Kernel of degree 'd' is given by K(xi, xj) = (γ * xi · xj + r)^d, where γ, r, and d are kernel parameters. This kernel effectively computes the dot product of the original feature vectors in the transformed space, implicitly representing the feature space transformation induced by a polynomial of degree 'd'.\n",
    "\n",
    "    By using polynomial kernels, SVM (or other kernel-based algorithms) can learn non-linear decision boundaries by working with polynomial features in higher-dimensional spaces without explicitly performing the computations in those spaces. This allows for more flexible and powerful machine learning models that can handle complex patterns and achieve better performance on non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0e140-88bb-4030-aeaa-f0c3f2ac0dc1",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn is straightforward. Scikit-learn provides a convenient library for SVM implementation, and it supports various kernel functions, including the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17183a2-322b-4317-93ab-6c467a6e1b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier with a polynomial kernel of degree 'd'\n",
    "svm_model = SVC(kernel='poly', degree=3)  # 'degree=3' is the degree of the polynomial kernel.\n",
    "\n",
    "# Train the SVM model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f50fc8-83fc-4c81-9e12-886890c853c5",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "    In Support Vector Regression (SVR), epsilon (ε) is a hyperparameter that defines the width of the tube around the regression line within which errors are ignored. It plays a crucial role in determining the number of support vectors and the complexity of the regression model.\n",
    "\n",
    "    Support vectors are the data points that lie on the boundary of the tube (also known as the ε-insensitive tube) or have prediction errors that fall within the tube. These data points have the most influence on defining the regression line and determining the model's accuracy. As epsilon is increased, the width of the ε-insensitive tube also increases.\n",
    "\n",
    "    Let's consider the effect of increasing the value of epsilon on the number of support vectors in SVR:\n",
    "\n",
    "    Small Epsilon (Narrow Tube):\n",
    "    When epsilon is small, the ε-insensitive tube is narrow, and the SVR model will be more sensitive to data points that have prediction errors within this narrow region. Consequently, the model will attempt to minimize the errors for these data points, potentially leading to more support vectors. In this case, the model might try to fit more closely to individual data points, and the regression line could be more flexible or prone to overfitting.\n",
    "\n",
    "    Large Epsilon (Wide Tube):\n",
    "    As epsilon is increased, the ε-insensitive tube becomes wider, allowing data points with larger prediction errors to be considered within the margin of tolerance. With a wider margin, the SVR model becomes less sensitive to individual data points and focuses more on capturing the general trend or average behavior of the data. Consequently, the model may have fewer support vectors compared to a model with a smaller epsilon.\n",
    "\n",
    "    Very Large Epsilon (No Tube):\n",
    "    In extreme cases where epsilon is set to a very large value (even larger than the actual prediction errors), the ε-insensitive tube essentially becomes a \"no-tube.\" All data points will fall within this margin of tolerance, and the SVR model will effectively behave like a traditional linear regression model. In this scenario, almost all data points may be considered as support vectors, leading to a larger number of support vectors.\n",
    "\n",
    "    In summary, increasing the value of epsilon in SVR results in a wider ε-insensitive tube, which makes the model more tolerant to prediction errors. This increased tolerance can lead to fewer support vectors since more data points are considered to have acceptable errors. Conversely, smaller values of epsilon result in a narrower ε-insensitive tube, making the model more sensitive to individual data points' errors, and potentially leading to more support vectors. The choice of epsilon should be carefully tuned based on the specific dataset and the desired trade-off between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9293c-044a-4b6c-b634-dd019e6c0ca1",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "    The performance of Support Vector Regression (SVR) is heavily influenced by its hyperparameters, including the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Let's explain each parameter and discuss how they affect the SVR performance, along with examples of when you might want to increase or decrease their values:\n",
    "\n",
    "    Kernel Function:\n",
    "    The kernel function is a crucial component of SVR that allows the algorithm to implicitly transform the data into a higher-dimensional feature space, enabling the modeling of non-linear relationships. The choice of the kernel function determines the shape of the decision boundary or regression line. Common kernel functions include the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel.\n",
    "    Linear Kernel: K(xi, xj) = xi · xj (No transformation, linear decision boundary).\n",
    "    Polynomial Kernel: K(xi, xj) = (γ * xi · xj + r)^d (Transformation to polynomial feature space).\n",
    "    RBF Kernel: K(xi, xj) = exp(-γ * ||xi - xj||^2) (Transformation to infinite-dimensional Gaussian feature space).\n",
    "    Sigmoid Kernel: K(xi, xj) = tanh(γ * xi · xj + r) (Transformation to infinite-dimensional hyperbolic tangent feature space).\n",
    "    Example: If your data has non-linear relationships between features, and a linear model is not sufficient to capture the underlying patterns, you might want to choose the polynomial or RBF kernel to achieve a more flexible and accurate regression model.\n",
    "\n",
    "    C Parameter (Regularization parameter):\n",
    "    The C parameter in SVR is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the prediction errors (residuals). A smaller C value allows more margin violations (large residuals), leading to a wider margin and potentially more support vectors. On the other hand, a larger C value penalizes margin violations more severely, resulting in a smaller margin and potentially fewer support vectors.\n",
    "    Example: If you have noisy data or outliers, a smaller C value can make the model more robust by allowing more tolerance for errors and leading to a wider margin. However, if you have a relatively clean dataset and want a more accurate model, you might consider using a larger C value to minimize prediction errors and obtain a narrower margin.\n",
    "\n",
    "    Epsilon Parameter (Tube Width):\n",
    "    The epsilon parameter (ε) is the width of the ε-insensitive tube around the regression line in SVR. Data points with prediction errors falling within this tube are ignored during training. A larger epsilon value increases the width of the tube, allowing more data points to be considered within the margin of tolerance, potentially resulting in fewer support vectors.\n",
    "    Example: If your data is noisy and you expect some variation in the target variable, you might use a larger epsilon value to allow for more tolerance in the errors and obtain a more robust model.\n",
    "\n",
    "    Gamma Parameter (Kernel Coefficient):\n",
    "    The gamma parameter is specific to the RBF kernel and determines the influence of a single training example. A small gamma value makes the influence of each training example more widespread (effectively smoothing the decision boundary), while a large gamma value makes the influence more localized (resulting in a more complex decision boundary).\n",
    "    Example: If you have a large dataset, a smaller gamma value can help avoid overfitting by creating a smoother decision boundary. For smaller datasets or when you suspect strong patterns in the data, a larger gamma value might be suitable to create more localized decision boundaries and capture finer details.\n",
    "\n",
    "    In summary, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter in SVR significantly affects the model's performance and generalization capability. Proper tuning of these hyperparameters is essential to achieve the best performance on your specific dataset. This can be done using techniques such as cross-validation or grid search to find the optimal combination of hyperparameters for your SVR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc63789-670e-4f97-a3b5-6f129d13379c",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "     Import the necessary libraries and load the dataset\n",
    "\n",
    "     Split the dataset into training and testing sets\n",
    "\n",
    "     Preprocess the data using any technique of your choice (e.g. scaling, normalizations\n",
    "\n",
    "     Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "     use the trained classifier to predict the labels of the testing data\n",
    "\n",
    "     Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-scores)\n",
    "\n",
    "     Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "\n",
    "     Train the tuned classifier on the entire dataset\n",
    "\n",
    "     Save the trained classifier to a file for future use.\n",
    "\n",
    "You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
    "classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2425d7-d7e4-4942-8e15-b4d57f440293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f0f853-0154-4674-abaf-09881046b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the make_classification dataset with 1000 samples, 20 features, and 2 classes\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into a 70-30 ratio for training and testing respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a470311a-4f20-42b4-b727-fb9e90f29d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240c8566-4ccf-43a1-b731-50690128940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SVC classifier with default hyperparameters\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the scaled training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a951fe-2b9d-4a52-ab44-cbabe65a1239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8433333333333334\n",
      "Precision: 0.8698630136986302\n",
      "Recall: 0.8193548387096774\n",
      "F1-score: 0.8438538205980065\n"
     ]
    }
   ],
   "source": [
    "# Calculate various metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54114b98-c0b2-42ef-9a4f-eb88b33955ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create an instance of the GridSearchCV with the SVC classifier\n",
    "grid_search = GridSearchCV(estimator=svc_classifier, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02698a4b-b156-4668-a1fe-704f50aa7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the tuned SVC classifier with the best parameters\n",
    "tuned_svc_classifier = SVC(**best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire scaled dataset\n",
    "tuned_svc_classifier.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726f2d9-d89b-45d3-98b9-a89262ddd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc_classifier, 'tuned_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004f05b-07b5-49b9-b8d0-8149fa730236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6fede-f14e-4238-966d-5c009fb0167b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b56ac6-56dc-4111-88ae-d85d86f45dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b108e-45b7-42e2-8fa8-aa299170149a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481ba9a-9363-497e-88d7-caf803b2befd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad75b04-844d-4079-b2e3-fc1c663c1365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2798545-5fe0-4661-b764-81a868522fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
