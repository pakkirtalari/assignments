Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.

    Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.
    Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. The scraper, on the other hand, is a specific tool created to extract data from the website. The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.
    Web scraping typically extracts large amounts of data from websites for a variety of uses such as price monitoring, enriching machine learning models, financial data aggregation, monitoring consumer sentiment, news tracking, etc. Browsers show data from a website.
       
Q2. What are the different methods used for Web Scraping?

    Web scrapers can extract all of the data on a specific site or the data that a user desires. Ideally, you should specify the data you want so that the web scraper extracts only that data quickly.
    For example, you may want to scrape an Amazon page for the different types of juicers available, but you may only want information about the models of different juicers and not customer reviews.
    When a web scraper needs to scrape a site, the URLs are provided first. The scraper then loads all of the HTML code for those sites, and a more advanced scraper may even extract all of the CSS and Javascript elements.
    The scraper then extracts the necessary data from the HTML code and outputs it in the format specified by the user. The data is typically saved in the form of an Excel spreadsheet or a CSV file, but it can also be saved in other formats, such as a JSON file.
    
Q3. What is Beautiful Soup? Why is it used?

    Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.
    Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don't have to think about encodings.
    
Q4. Why is flask used in this Web Scraping project?

    Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library.

Q5. Write the names of AWS services used in this project. Also, explain the use of each service.

    1. AWS Elastic Beanstalk:-
        Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring.
        With AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.
    
    2. AWS CodePipeline:- 
        AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.
        AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.
        A pipeline must contain at least two stages. The first stage of a pipeline must contain at least one source action. It can contain source actions only. Only the first stage of a pipeline can contain source actions.
