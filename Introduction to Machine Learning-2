Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?

    Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data. (It’s just like trying to fit undersized pants!) Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. In such cases, the rules of the machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions. 
    Underfitting can be avoided by using more data and also reducing the features by feature selection.
    Reasons for Underfitting:
      High bias and low variance 
      The size of the training dataset used is not enough.
      The model is too simple.
      Training data is not cleaned and also contains noise in it.
    Techniques to reduce underfitting: 
      Increase model complexity
      Increase the number of features, performing feature engineering
      Remove noise from the data.
      Increase the number of epochs or increase the duration of training to get better results.
      
    Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. 
    When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. 
    A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. 
    Reasons for Overfitting are as follows:
      High variance and low bias 
      The model is too complex
      The size of the training data 
    Techniques to reduce overfitting:
      Increase training data.
      Reduce model complexity.
      Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).
      Ridge Regularization and Lasso Regularization
      Use dropout for neural networks to tackle overfitting.
    
Q2: How can we reduce overfitting? Explain in brief.

    Techniques to reduce overfitting:
      Increase training data.
      Reduce model complexity.
      Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).
      Ridge Regularization and Lasso Regularization
      Use dropout for neural networks to tackle overfitting.

Q3: Explain underfitting. List scenarios where underfitting can occur in ML.

    Reasons for Underfitting:
      High bias and low variance 
      The size of the training dataset used is not enough.
      The model is too simple.
      Training data is not cleaned and also contains noise in it.
    Techniques to reduce underfitting: 
      Increase model complexity
      Increase the number of features, performing feature engineering
      Remove noise from the data.
      Increase the number of epochs or increase the duration of training to get better results.

Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?

    Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. 
    Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.
    
    Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. 
    Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.
    
    If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.
    This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.
    
Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.
How can you determine whether your model is overfitting or underfitting?

    Detecting Overfitting or Underfitting
      There are a few ways we can understand how to "diagnose" underfitting and overfitting.  

      Underfitting occurs when your model produces accurate but inaccurate predictions at first. In this scenario, the training error is substantial, as is the validation/test error.  

      Overfitting occurs when your model fails to generate correct predictions. The training error is relatively modest in this example, but the validation/test error is highly significant.  

      When you identify a decent model, the training error is small (albeit more significant than in the case of overfitting), and the validation/test error is also minimal.  

      It would help if you remembered as a general intuition that underfitting arises when your model is too simplistic for your data. Conversely, overfitting happens when your model is too complicated for your data. 
    

Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias
and high variance models, and how do they differ in terms of their performance?

    Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. 
    Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.
    Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. 
    Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. 
    As a result, such models perform very well on training data but has high error rates on test data.
    If our model is too simple and has very few parameters then it may have high bias and low variance. 
    On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. 
    So we need to find the right/good balance without overfitting and underfitting the data.
    This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.


Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe
some common regularization techniques and how they work.

    Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.
    Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it. 
    Regularization Techniques 
        There are two main types of regularization techniques: Ridge Regularization and Lasso Regularization.
    Ridge Regularization : 
      Also known as Ridge Regression, it modifies the over-fitted or under fitted models by adding the penalty equivalent to the sum of the squares of the magnitude of coefficients.
      This means that the mathematical function representing our machine learning model is minimized and coefficients are calculated. The magnitude of coefficients is squared and added. Ridge Regression performs regularization by shrinking the coefficients present.
    Lasso Regression :
      It modifies the over-fitted or under-fitted models by adding the penalty equivalent to the sum of the absolute values of coefficients. 
      Lasso regression also performs coefficient minimization,  but instead of squaring the magnitudes of the coefficients, it takes the true values of coefficients. This means that the coefficient sum can also be 0, because of the presence of negative coefficients. 
    
