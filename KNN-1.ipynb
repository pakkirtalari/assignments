{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0434b72d-684b-4dbd-b5e8-a653913bac56",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "A1. K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It works based on the idea that data points with similar features tend to have similar labels. In KNN, for a given input data point, the algorithm identifies the K closest data points in the training set (neighbors) and predicts the label of the input point based on the majority class (classification) or the average (regression) of the labels of its neighbors.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "A2. The value of K is a hyperparameter in KNN that determines the number of neighbors used for prediction. The choice of K depends on the dataset and problem at hand. A small K can lead to noise sensitivity, while a large K can lead to oversmoothing. One common approach is to use techniques like cross-validation to find the optimal K that gives the best performance on validation data.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "A3. The main difference between KNN classifier and KNN regressor lies in their task:\n",
    "\n",
    "KNN Classifier: In classification tasks, KNN classifier predicts the class label of a data point by taking the majority class among its K nearest neighbors.\n",
    "KNN Regressor: In regression tasks, KNN regressor predicts a numerical value by taking the average (or weighted average) of the target values of its K nearest neighbors.\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "A4. The performance of KNN can be measured using appropriate evaluation metrics for the task:\n",
    "\n",
    "KNN Classification: Common metrics include accuracy, precision, recall, F1-score, and ROC curves.\n",
    "KNN Regression: Common metrics include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "A5. The \"curse of dimensionality\" refers to the phenomenon where the performance of KNN deteriorates as the number of dimensions (features) increases. In high-dimensional spaces, data points become more equidistant, making it harder to distinguish nearest neighbors effectively. This can lead to reduced predictive accuracy and increased computational complexity.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "A6. Handling missing values in KNN can be challenging. One approach is to impute missing values before applying KNN. Common methods include replacing missing values with the mean, median, or a specific value. Another approach is to assign a special value to missing entries and treat them as a distinct category. More advanced techniques involve using other features to predict missing values.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "A7. KNN classifier is better suited for problems where the output is categorical (classification tasks), while KNN regressor is better suited for problems where the output is numerical (regression tasks). The choice depends on the nature of the problem and the type of output you are predicting.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "A8. Strengths:\n",
    "Simple and intuitive.\n",
    "No assumption about data distribution.\n",
    "Can capture complex relationships.\n",
    "Weaknesses:\n",
    "\n",
    "Sensitive to irrelevant features and noisy data.\n",
    "Computationally expensive for large datasets.\n",
    "Curse of dimensionality in high-dimensional spaces.\n",
    "To address weaknesses, consider feature selection, data preprocessing, and dimensionality reduction techniques.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "A9. Euclidean distance is the straight-line distance between two points in Euclidean space. Manhattan distance (also known as L1 distance or city block distance) is the sum of the absolute differences between corresponding components of two points. Euclidean distance considers the actual distance between points, while Manhattan distance accounts for the distance along each axis.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "A10. Feature scaling is crucial in KNN because the algorithm's performance is highly influenced by the scale of features. Features with larger scales can dominate the distance calculations, leading to biased results. Scaling ensures that all features contribute equally to the distance computation, improving the algorithm's performance. Common scaling techniques include Min-Max scaling and Z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e7df8-e7b5-49f4-946d-18d84a0baff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
