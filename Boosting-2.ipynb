{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc7845b-21d0-4c4b-b42d-5a088964966f",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "A1. Gradient Boosting Regression is a machine learning algorithm that belongs to the family of boosting techniques. It's used for regression tasks and works by combining the predictions of multiple weak learners (usually decision trees) in an additive manner. The algorithm iteratively fits new weak learners to the residual errors of the predictions made by the ensemble so far, gradually improving the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda4ad9-a608-4922-9581-c1050931f850",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.\n",
    "Here's a simplified example of a gradient boosting regression algorithm from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3458e17e-3e93-42f3-9a73-22c1b5cd00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.94386586844897\n",
      "R-squared: 0.8651552868724599\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 100) * 10\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 2\n",
    "\n",
    "# Initialize parameters\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions with a constant value\n",
    "predictions = np.ones(y.shape) * np.mean(y)\n",
    "\n",
    "# Implement Gradient Boosting\n",
    "for _ in range(n_estimators):\n",
    "    residuals = y - predictions\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(X, residuals)\n",
    "    gradient = tree.predict(X)\n",
    "    predictions += learning_rate * gradient\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435cfd7-d9e5-4ea1-af3b-0de7dd64e410",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters to optimize the performance of the model.\n",
    "You can use libraries like GridSearchCV or RandomizedSearchCV from scikit-learn to perform hyperparameter tuning. Here's a high-level example using GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b7d3d-aad4-42b5-92bc-45d26fedcfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_reg, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da507c89-8295-4fee-846c-9eae039b5b03",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A4. A weak learner in Gradient Boosting is a model that performs slightly better than random chance. It's usually a simple model with low complexity, such as a decision tree with limited depth. Weak learners are often combined in an ensemble to gradually improve the model's performance.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "A5. The intuition behind Gradient Boosting is to build an ensemble of weak learners that work together to improve the predictions iteratively. Each weak learner corrects the errors made by the previous ensemble, focusing on the samples that are difficult to predict correctly. The ensemble converges towards a model that performs well on the training data.\n",
    "\n",
    "Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "A6. Gradient Boosting builds an ensemble by training a series of weak learners sequentially. It starts with an initial prediction (usually the mean of the target values) and fits a weak learner to the residual errors of the previous predictions. The new weak learner is added to the ensemble, and the process is repeated iteratively.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "A7. The mathematical intuition of Gradient Boosting involves the following steps:\n",
    "\n",
    "Initialize predictions with a constant value (e.g., mean of target values).\n",
    "Compute the residuals by subtracting the current predictions from the actual target values.\n",
    "Fit a weak learner (e.g., decision tree) to the residuals.\n",
    "Compute the predictions from the weak learner and multiply by a learning rate.\n",
    "Update the predictions by adding the learning rate times the weak learner's predictions.\n",
    "Repeat steps 2-5 for a specified number of iterations (n_estimators).\n",
    "The final ensemble prediction is the sum of all weak learners' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679f4b4-9321-4555-b984-9a15d960ccf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
